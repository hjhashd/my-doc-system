
import os, json, traceback, time
from typing import Optional, Dict, Any, List, Callable
from fastapi import FastAPI, BackgroundTasks, Query
from pydantic import BaseModel, Field

# ===== æ˜¾å¼å¸¸é‡å®šä¹‰ =====
DEFAULT_OUTPUT_FILE_PATH: str = "/data/cwd_cq/out"
DEFAULT_INPUT_FILE_PATH: str = "/home/xzh/ocr_flie/pdf_output"
DEFAULT_RUN_PICTURE: bool = True
DEFAULT_USE_MERGED_FOR_MERGE: bool = True

# ===== å¯¼å…¥ä½ çš„æºæ–‡ä»¶ =====
import title_recognition as mod_title_rec
import title_position_calculator as mod_pos
import clear_empty_blocks_manager as mod_clear
import table_recognition as mod_table
import table_title_completely_merge_with_content as mod_merge
from picture_recognition import EnhancedImageProcessor

# ===== æœåŠ¡å¸¸é‡ =====
RUN_HTTP_SERVER: bool = True
SERVER_HOST: str = "0.0.0.0"
SERVER_PORT: int = 8005
SERVER_WORKERS: int = 1

# ===== å…¨å±€ä»»åŠ¡çŠ¶æ€å­˜å‚¨ =====
GLOBAL_TASK_STORE: Dict[str, Dict[str, Any]] = {}

# ============== å·¥å…·å‡½æ•° ==============
def _abspath(p: Optional[str]) -> Optional[str]:
    return os.path.abspath(p) if p else None

def _assert_file_exists(p: str, name: str):
    if not os.path.isfile(p):
        raise FileNotFoundError(f"{name} ä¸å­˜åœ¨: {p}")

def _build_output_dir(output_file_path: str, agent_user_id: int, task_id: str) -> str:
    output_dir = os.path.join(output_file_path, str(agent_user_id), str(task_id))
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def _wrap_blocks_if_needed(merged_path: str, json_file_name: str, output_dir: str) -> str:
    with open(merged_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if isinstance(data, dict) and "blocks" in data:
        return merged_path
    if isinstance(data, list):
        wrapped = {"blocks": data}
        os.makedirs(output_dir, exist_ok=True)
        wrapped_path = os.path.join(output_dir, f"{json_file_name}_blocks_merge_wrapped.json")
        with open(wrapped_path, "w", encoding="utf-8") as wf:
            json.dump(wrapped, wf, ensure_ascii=False, indent=2)
        return _abspath(wrapped_path)
    return merged_path

# ============== ç¼“å­˜æ£€æŸ¥é€»è¾‘ ==============
def check_cache_result(req_data: 'PipelineRequest') -> Optional[Dict[str, Any]]:
    """
    æ£€æŸ¥è¾“å‡ºç›®å½•ä¸‹æ˜¯å¦å·²ç»å­˜åœ¨å…³é”®æ–‡ä»¶ã€‚
    å¦‚æœå­˜åœ¨ï¼Œç›´æ¥æ„é€ å¹¶è¿”å›æˆåŠŸçš„ result å­—å…¸ï¼›å¦åˆ™è¿”å› Noneã€‚
    """
    try:
        output_dir = _build_output_dir(req_data.output_file_path, req_data.agentUserId, req_data.task_id)
        file_base_name = os.path.splitext(req_data.file_name)[0]
        
        # æ£€æŸ¥æœ€å…³é”®çš„å®Œæˆæ ‡å¿—æ–‡ä»¶ (Step5)
        final_table_path = os.path.join(output_dir, f"{file_base_name}_tables_with_heading.json")
        if not os.path.exists(final_table_path):
            return None
            
        print(f"[Cache Hit] æ£€æµ‹åˆ°ç°æœ‰ç»“æœæ–‡ä»¶: {final_table_path}")

        def get_path_if_exists(suffix: str) -> Optional[str]:
            p = os.path.join(output_dir, f"{file_base_name}{suffix}")
            return _abspath(p) if os.path.exists(p) else None

        file_outputs = {
            "step1_title_recognition": {
                "step_name": "æ ‡é¢˜è¯†åˆ«", 
                "file_path": get_path_if_exists(".json"),
                "file_name": f"{file_base_name}.json",
                "description": "è¯†åˆ«æ–‡æ¡£ä¸­çš„æ ‡é¢˜åŠå…¶å±‚çº§ç»“æ„"
            },
            "step2_title_position_blocks": {
                "step_name": "æ ‡é¢˜å®šä½ä¸åˆ‡å—", 
                "file_path": get_path_if_exists("_blocks.json"), 
                "file_name": f"{file_base_name}_blocks.json",
                "description": "æ ¹æ®æ ‡é¢˜ä½ç½®å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªå—"
            },
            "step3_blocks_merge": {
                "step_name": "ç©ºå—åˆå¹¶", 
                "file_path": get_path_if_exists("_blocks_merge.json"), 
                "file_name": f"{file_base_name}_blocks_merge.json",
                "description": "åˆå¹¶æ–‡æ¡£ä¸­çš„ç©ºç™½å—"
            },
            "step3_blocks_merge_wrapped": {
                "step_name": "ç©ºå—åˆå¹¶ï¼ˆåŒ…è£…ç‰ˆï¼‰", 
                "file_path": get_path_if_exists("_blocks_merge_wrapped.json"),
                "file_name": f"{file_base_name}_blocks_merge_wrapped.json",
                "description": "åŒ…è£…åçš„ç©ºå—åˆå¹¶ç»“æœ"
            },
            "step4_table_recognition": {
                "step_name": "è¡¨æ ¼è¯†åˆ«", 
                "file_path": get_path_if_exists("_tables.json"), 
                "file_name": f"{file_base_name}_tables.json",
                "description": "è¯†åˆ«æ–‡æ¡£ä¸­çš„æ‰€æœ‰è¡¨æ ¼åŠå…¶å†…å®¹"
            },
            "step5_tables_with_heading": {
                "step_name": "è¡¨æ ¼æ ‡é¢˜åˆå¹¶", 
                "file_path": get_path_if_exists("_tables_with_heading.json"), 
                "file_name": f"{file_base_name}_tables_with_heading.json",
                "description": "å°†æ ‡é¢˜å…ƒæ•°æ®åˆå¹¶åˆ°è¡¨æ ¼ä¿¡æ¯ä¸­"
            },
            "step6_picture_recognition": {
                "step_name": "å›¾ç‰‡è¯†åˆ«", 
                "file_path": get_path_if_exists("_picture.json"), 
                "file_name": f"{file_base_name}_picture.json",
                "description": "è¯†åˆ«æ–‡æ¡£ä¸­çš„å›¾ç‰‡åŠå…³é”®ä¿¡æ¯"
            }
        }

        return {
            "ok": True, 
            "status": 1, 
            "message": "æ£€æµ‹åˆ°å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œè·³è¿‡å¤„ç†ç›´æ¥è¿”å›ç»“æœ (Cache Hit)",
            "file_outputs": file_outputs, 
            "meta": {
                "document_name": file_base_name, 
                "task_id": req_data.task_id, 
                "agent_user_id": req_data.agentUserId, 
                "output_directory": output_dir,
                "input_file_path": req_data.input_file_path
            }, 
            "log": ["âœ… [Cache] æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æµæ°´çº¿æ‰§è¡Œã€‚"],
            "config": {"output_directory": output_dir}
        }
    
    except Exception as e:
        print(f"[Cache Error] æ£€æŸ¥ç¼“å­˜æ—¶å‡ºé”™: {e}")
        return None

# ============== ğŸ”¥ å®Œæ•´æ ¸å¿ƒæµç¨‹ï¼ˆä»åŸæ–‡ä»¶æ¢å¤ï¼‰ ğŸ”¥ ==============
def run_pipeline_sync(
        file_path: str,
        task_id: str,
        agent_user_id: int,
        output_file_path: str,
        input_file_path: str = DEFAULT_INPUT_FILE_PATH,
        run_picture: bool = DEFAULT_RUN_PICTURE,
        use_merged_blocks_for_merge: bool = DEFAULT_USE_MERGED_FOR_MERGE,
        progress_callback: Callable[[str, int, str], None] = None
) -> Dict[str, Any]:
    def report_progress(step_key: str, percent: int, desc: str):
        if progress_callback:
            progress_callback(step_key, percent, desc)

    output_dir = _build_output_dir(output_file_path, agent_user_id, task_id)
    
    file_outputs = {
        "step1_title_recognition": {"step_name": "æ ‡é¢˜è¯†åˆ«", "file_path": None, "file_name": None, "description": ""},
        "step2_title_position_blocks": {"step_name": "æ ‡é¢˜å®šä½ä¸åˆ‡å—", "file_path": None, "file_name": None, "description": ""},
        "step3_blocks_merge": {"step_name": "ç©ºå—åˆå¹¶", "file_path": None, "file_name": None, "description": ""},
        "step3_blocks_merge_wrapped": {"step_name": "ç©ºå—åˆå¹¶ï¼ˆåŒ…è£…ç‰ˆï¼‰", "file_path": None, "file_name": None, "description": ""},
        "step4_table_recognition": {"step_name": "è¡¨æ ¼è¯†åˆ«", "file_path": None, "file_name": None, "description": ""},
        "step5_tables_with_heading": {"step_name": "è¡¨æ ¼æ ‡é¢˜åˆå¹¶", "file_path": None, "file_name": None, "description": ""},
        "step6_picture_recognition": {"step_name": "å›¾ç‰‡è¯†åˆ«", "file_path": None, "file_name": None, "description": ""}
    }
    
    meta = {
        "document_name": None,
        "task_id": task_id,
        "agent_user_id": agent_user_id,
        "output_directory": output_dir,
        "input_file_path": input_file_path
    }
    
    log = []
    print("u"*40)
    
    try:
        report_progress("init", 5, "åˆå§‹åŒ–ä»»åŠ¡")
        
        _assert_file_exists(file_path, "DOCXæ–‡ä»¶")
        log.append(f"è¾“å…¥æ–‡ä»¶: {file_path}")
        log.append(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # Step 1: æ ‡é¢˜è¯†åˆ«
        report_progress("step1", 15, "æ‰§è¡Œæ ‡é¢˜è¯†åˆ«...")
        log.append("Step 1: æ ‡é¢˜è¯†åˆ«")
        text, json_file_name, title_json_path = mod_title_rec.main(file_path, output_dir=output_dir)
        if text is None or not json_file_name or not title_json_path:
            raise RuntimeError("Step 1 æ ‡é¢˜è¯†åˆ«å¤±è´¥")
        file_outputs["step1_title_recognition"]["file_path"] = _abspath(title_json_path)
        file_outputs["step1_title_recognition"]["file_name"] = os.path.basename(title_json_path)
        meta["document_name"] = json_file_name
        log.append(f"âœ“ Step 1 å®Œæˆ: {title_json_path}")
        
        # Step 2: æ ‡é¢˜å®šä½ä¸åˆ‡å—
        report_progress("step2", 30, "æ‰§è¡Œæ ‡é¢˜å®šä½ä¸åˆ‡å—...")
        log.append("Step 2: æ ‡é¢˜å®šä½ä¸åˆ‡å—")
        blocks_json_path = mod_pos.main(text, title_json_path, output_dir=output_dir)
        if not blocks_json_path:
            raise RuntimeError("Step 2 æ ‡é¢˜å®šä½å¤±è´¥")
        file_outputs["step2_title_position_blocks"]["file_path"] = _abspath(blocks_json_path)
        file_outputs["step2_title_position_blocks"]["file_name"] = os.path.basename(blocks_json_path)
        log.append(f"âœ“ Step 2 å®Œæˆ: {blocks_json_path}")
        
        # Step 3: ç©ºå—åˆå¹¶
        report_progress("step3", 45, "æ‰§è¡Œç©ºå—åˆå¹¶...")
        log.append("Step 3: ç©ºå—åˆå¹¶")
        blocks_merge_json_path = mod_clear.main(
            json_file_name=json_file_name, 
            input_blocks_path=blocks_json_path, 
            output_dir=output_dir
        )
        blocks_for_merge = file_outputs["step2_title_position_blocks"]["file_path"]
        if blocks_merge_json_path:
            file_outputs["step3_blocks_merge"]["file_path"] = _abspath(blocks_merge_json_path)
            file_outputs["step3_blocks_merge"]["file_name"] = os.path.basename(blocks_merge_json_path)
            if use_merged_blocks_for_merge and blocks_merge_json_path:
                wrapped_path = _wrap_blocks_if_needed(blocks_merge_json_path, json_file_name, output_dir)
                file_outputs["step3_blocks_merge_wrapped"]["file_path"] = wrapped_path
                file_outputs["step3_blocks_merge_wrapped"]["file_name"] = os.path.basename(wrapped_path)
                blocks_for_merge = wrapped_path
        log.append(f"âœ“ Step 3 å®Œæˆ: {blocks_merge_json_path}")
        
        # Step 4: è¡¨æ ¼è¯†åˆ«
        report_progress("step4", 60, "æ‰§è¡Œè¡¨æ ¼è¯†åˆ«...")
        log.append("Step 4: è¡¨æ ¼è¯†åˆ«")
        tables_json_path = mod_table.main(
            text, json_file_name, output_dir=output_dir, 
            input_file_path=input_file_path, 
            agent_user_id=agent_user_id, task_id=task_id
        )
        if not tables_json_path:
            raise RuntimeError("Step 4 è¡¨æ ¼è¯†åˆ«å¤±è´¥")
        file_outputs["step4_table_recognition"]["file_path"] = _abspath(tables_json_path)
        file_outputs["step4_table_recognition"]["file_name"] = os.path.basename(tables_json_path)
        log.append(f"âœ“ Step 4 å®Œæˆ: {tables_json_path}")
        
        # Step 5: è¡¨æ ¼æ ‡é¢˜åˆå¹¶
        report_progress("step5", 75, "æ‰§è¡Œè¡¨æ ¼æ ‡é¢˜åˆå¹¶...")
        log.append("Step 5: è¡¨æ ¼æ ‡é¢˜åˆå¹¶")
        tables_with_heading_json_path = mod_merge.main(
            json_file_name, tables_json_path, 
            blocks_json_path=blocks_for_merge, 
            output_dir=output_dir
        )
        if not tables_with_heading_json_path:
            raise RuntimeError("Step 5 è¡¨æ ¼æ ‡é¢˜åˆå¹¶å¤±è´¥")
        file_outputs["step5_tables_with_heading"]["file_path"] = _abspath(tables_with_heading_json_path)
        file_outputs["step5_tables_with_heading"]["file_name"] = os.path.basename(tables_with_heading_json_path)
        log.append(f"âœ“ Step 5 å®Œæˆ: {tables_with_heading_json_path}")
        
        # Step 6: å›¾ç‰‡è¯†åˆ«
        report_progress("step6", 90, "æ‰§è¡Œå›¾ç‰‡è¯†åˆ«...")
        if run_picture:
            log.append("Step 6: å›¾ç‰‡è¯†åˆ«")
            proc = EnhancedImageProcessor(input_file_path, agent_user_id, task_id)
            results = proc.process_text_with_images(text)
            picture_json_path = proc.save_results(results, output_file=f"{json_file_name}_picture.json", output_dir=output_dir)
            file_outputs["step6_picture_recognition"]["file_path"] = _abspath(picture_json_path)
            file_outputs["step6_picture_recognition"]["file_name"] = os.path.basename(picture_json_path)
            log.append(f"âœ“ Step 6 å®Œæˆ: {picture_json_path}")
        else:
            log.append("Step 6: è·³è¿‡å›¾ç‰‡è¯†åˆ«")
        
        report_progress("finished", 100, "æ‰€æœ‰æ­¥éª¤å®Œæˆ")
        log.append("âœ… æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
        
        return {
            "ok": True, "status": 1, "message": "å¤„ç†æˆåŠŸ", 
            "file_outputs": file_outputs, "meta": meta, "log": log, 
            "config": {"output_directory": output_dir}
        }
        
    except Exception as e:
        log.append(f"âŒ é”™è¯¯: {type(e).__name__}: {e}")
        file_outputs["meta"] = meta
        file_outputs["log"] = log + [traceback.format_exc()]
        file_outputs["config"] = {"output_directory": output_dir}
        return {
            "ok": False, "status": 0, "message": f"{type(e).__name__}: {e}", 
            "file_outputs": file_outputs, "meta": meta, "log": log,
            "config": {"output_directory": output_dir}
        }

# ============== FastAPI æ¥å£ ==============
app = FastAPI(title="Doc Pipeline API", version="1.8.0", docs_url="/docs")

class PipelineRequest(BaseModel):
    file_name: str = Field(..., description="æ–‡ä»¶åç§° (ä¾‹å¦‚ test.docx)", example="è¯ä¹¦æ–‡æ¡£.docx")
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    agentUserId: int = Field(..., description="ä»£ç†ç”¨æˆ·ID")
    output_file_path: str = Field(default=DEFAULT_OUTPUT_FILE_PATH, description="ç»“æœè¾“å‡ºç›®å½•")
    input_file_path: str = Field(default=DEFAULT_INPUT_FILE_PATH, description="æ–‡ä»¶æ‰€åœ¨çš„æ ¹ç›®å½•")

class RunResponse(BaseModel):
    ok: bool
    message: str
    query_id: str
    status_url: str

class StatusResponse(BaseModel):
    ok: bool
    status: str
    percent: int
    message: str
    result: Optional[Dict[str, Any]] = None

# ============== åå°ä»»åŠ¡åŒ…è£…å™¨ ==============
def background_process_wrapper(req_data: PipelineRequest, unique_key: str):
    GLOBAL_TASK_STORE[unique_key] = {
        "status": "running", "percent": 0, "current_step": "init",
        "message": "ä»»åŠ¡å·²å¯åŠ¨", "result": None
    }

    def update_store(step_key, percent, desc):
        if unique_key in GLOBAL_TASK_STORE:
            GLOBAL_TASK_STORE[unique_key]["percent"] = percent
            GLOBAL_TASK_STORE[unique_key]["current_step"] = step_key
            GLOBAL_TASK_STORE[unique_key]["message"] = desc

    try:
        full_file_path = os.path.join(req_data.input_file_path, str(req_data.task_id), req_data.file_name)
        print(f"Processing file: {full_file_path}")
        
        final_result = run_pipeline_sync(
            file_path=full_file_path,
            task_id=req_data.task_id,
            agent_user_id=req_data.agentUserId,
            output_file_path=req_data.output_file_path,
            input_file_path=req_data.input_file_path,
            run_picture=DEFAULT_RUN_PICTURE,
            use_merged_blocks_for_merge=DEFAULT_USE_MERGED_FOR_MERGE,
            progress_callback=update_store
        )

        status_str = "success" if final_result["status"] == 1 else "failed"
        GLOBAL_TASK_STORE[unique_key].update({
            "status": status_str, "percent": 100, "current_step": "finished",
            "message": final_result["message"], "result": final_result
        })

    except Exception as e:
        traceback.print_exc()
        GLOBAL_TASK_STORE[unique_key].update({
            "status": "failed", "percent": 100, 
            "message": f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}", "result": None
        })

@app.post("/pipeline/run", response_model=RunResponse)
async def run_pipeline(req: PipelineRequest, background_tasks: BackgroundTasks):
    unique_key = f"{req.agentUserId}_{req.task_id}"
    
    if unique_key in GLOBAL_TASK_STORE and GLOBAL_TASK_STORE[unique_key]["status"] == "running":
        return {
            "ok": True, "message": "ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­", "query_id": unique_key,
            "status_url": f"/pipeline/status?query_id={unique_key}"
        }
    
    background_tasks.add_task(background_process_wrapper, req, unique_key)
    return {
        "ok": True, "message": "ä»»åŠ¡å·²æ¥æ”¶ï¼Œæ­£åœ¨åå°å¤„ç†", "query_id": unique_key,
        "status_url": f"/pipeline/status?query_id={unique_key}"
    }

@app.post("/pipeline/run_check", response_model=RunResponse)
async def run_pipeline_check(req: PipelineRequest, background_tasks: BackgroundTasks):
    """
    æ™ºèƒ½æ¥å£ï¼šä¼˜å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œæœ‰ç¼“å­˜ç›´æ¥è¿”å›ï¼Œæ— ç¼“å­˜æ‰è¿è¡Œ
    """
    unique_key = f"{req.agentUserId}_{req.task_id}"
    
    cached_result = check_cache_result(req)
    if cached_result:
        GLOBAL_TASK_STORE[unique_key] = {
            "status": "success", "percent": 100, "current_step": "finished",
            "message": "æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç† (Cached)", "result": cached_result
        }
        return {
            "ok": True, "message": "æ£€æµ‹åˆ°æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ— éœ€å¤„ç†", 
            "query_id": unique_key,
            "status_url": f"/pipeline/status?query_id={unique_key}"
        }

    if unique_key in GLOBAL_TASK_STORE and GLOBAL_TASK_STORE[unique_key]["status"] == "running":
        return {
            "ok": True, "message": "ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­", "query_id": unique_key,
            "status_url": f"/pipeline/status?query_id={unique_key}"
        }

    background_tasks.add_task(background_process_wrapper, req, unique_key)
    return {
        "ok": True, "message": "æœªæ£€æµ‹åˆ°ç»“æœï¼Œä»»åŠ¡å·²å¯åŠ¨åå°å¤„ç†", 
        "query_id": unique_key,
        "status_url": f"/pipeline/status?query_id={unique_key}"
    }

@app.get("/pipeline/status", response_model=StatusResponse)
async def get_pipeline_status(query_id: str = Query(..., description="ä»»åŠ¡çš„å”¯ä¸€ID")):
    task_info = GLOBAL_TASK_STORE.get(query_id)
    if not task_info:
        return {"ok": False, "status": "not_found", "percent": 0, "message": "ä»»åŠ¡ä¸å­˜åœ¨", "result": None}
    return {
        "ok": True, "status": task_info["status"], "percent": task_info["percent"],
        "message": task_info["message"], "result": task_info["result"]
    }

class StatisticsRequest(BaseModel):
    file_name: str = Field(..., description="æ–‡ä»¶åç§° (ä¾‹å¦‚ test.docx)", example="è¯ä¹¦æ–‡æ¡£.docx")
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    agentUserId: int = Field(..., description="ä»£ç†ç”¨æˆ·ID")
    output_file_path: str = Field(default=DEFAULT_OUTPUT_FILE_PATH, description="ç»“æœè¾“å‡ºç›®å½•")
    input_file_path: str = Field(default=DEFAULT_INPUT_FILE_PATH, description="æ–‡ä»¶æ‰€åœ¨çš„æ ¹ç›®å½•")

class StatisticsResponse(BaseModel):
    ok: bool
    message: str
    statistics: Optional[Dict[str, Any]] = None

def get_document_statistics(file_path: str, task_id: str, agent_user_id: int, output_file_path: str):
    """
    è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬å—ã€è¡¨æ ¼ã€å›¾ç‰‡æ•°é‡ï¼Œä»¥åŠæ–‡ä»¶å¤§å°ã€ä¸Šä¼ æ—¥æœŸã€æ€»é¡µæ•°ç­‰
    """
    try:
        # è·å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        file_name = os.path.basename(file_path)
        file_name_without_ext = os.path.splitext(file_name)[0]
        
        # æ„å»ºè¾“å‡ºç›®å½•è·¯å¾„
        output_dir = os.path.join(output_file_path, str(agent_user_id), task_id)
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        statistics = {
            "file_name": file_name,
            "file_path": file_path,
            "text_blocks_count": 0,
            "tables_count": 0,
            "images_count": 0,
            "total_pages": 0,
            "file_size": 0,
            "upload_date": "",
            "text_recognition_rate": "æš‚æœªå®ç°",
            "structure_restoration_degree": "æš‚æœªå®ç°",
            "anomaly_items": "æš‚æœªå®ç°"
        }
        
        # è·å–æ–‡ä»¶å¤§å°å’Œä¸Šä¼ æ—¥æœŸ
        if os.path.exists(file_path):
            file_stat = os.stat(file_path)
            statistics["file_size"] = file_stat.st_size
            # è½¬æ¢ä¸ºKB
            statistics["file_size_kb"] = round(file_stat.st_size / 1024, 2)
            # è·å–æ–‡ä»¶ä¿®æ”¹æ—¥æœŸä½œä¸ºä¸Šä¼ æ—¥æœŸ
            statistics["upload_date"] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_stat.st_mtime))
        
        # è¯»å–æ–‡æœ¬å—ä¿¡æ¯
        blocks_file_path = os.path.join(output_dir, f"{file_name_without_ext}_blocks.json")
        if os.path.exists(blocks_file_path):
            with open(blocks_file_path, 'r', encoding='utf-8') as f:
                blocks_data = json.load(f)
                if "total_blocks" in blocks_data:
                    statistics["text_blocks_count"] = blocks_data["total_blocks"]
                elif "blocks" in blocks_data:
                    statistics["text_blocks_count"] = len(blocks_data["blocks"])
        
        # è¯»å–è¡¨æ ¼ä¿¡æ¯
        tables_file_path = os.path.join(output_dir, f"{file_name_without_ext}_tables_with_heading.json")
        if os.path.exists(tables_file_path):
            with open(tables_file_path, 'r', encoding='utf-8') as f:
                tables_data = json.load(f)
                if "tables" in tables_data:
                    statistics["tables_count"] = len(tables_data["tables"])
        
        # è¯»å–å›¾ç‰‡ä¿¡æ¯
        picture_file_path = os.path.join(output_dir, f"{file_name_without_ext}_picture.json")
        if os.path.exists(picture_file_path):
            with open(picture_file_path, 'r', encoding='utf-8') as f:
                picture_data = json.load(f)
                if "total_images" in picture_data:
                    statistics["images_count"] = picture_data["total_images"]
                elif "results" in picture_data:
                    statistics["images_count"] = len(picture_data["results"])
        
        # å°è¯•ä»æ–‡æœ¬å—ä¿¡æ¯ä¸­è·å–é¡µæ•°ä¿¡æ¯
        if os.path.exists(blocks_file_path):
            with open(blocks_file_path, 'r', encoding='utf-8') as f:
                blocks_data = json.load(f)
                if "blocks" in blocks_data and blocks_data["blocks"]:
                    # ä»æœ€åä¸€ä¸ªblockè·å–æœ€å¤§è¡Œå·ä½œä¸ºæ€»é¡µæ•°
                    max_line = 0
                    for block in blocks_data["blocks"]:
                        if "line_end" in block and block["line_end"] > max_line:
                            max_line = block["line_end"]
                    # è¿™é‡Œå‡è®¾æ¯é¡µå¤§çº¦æœ‰40è¡Œï¼Œè¿™æ˜¯ä¸€ä¸ªç²—ç•¥ä¼°è®¡
                    statistics["total_pages"] = max(1, round(max_line / 40))
        
        return {"ok": True, "message": "ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ", "statistics": statistics}
    
    except Exception as e:
        return {"ok": False, "message": f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}", "statistics": None}

@app.post("/pipeline/statistics", response_model=StatisticsResponse)
async def get_statistics(req: StatisticsRequest):
    """
    è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯æ¥å£
    """
    full_file_path = os.path.join(req.input_file_path, str(req.task_id), req.file_name)
    
    if not os.path.exists(full_file_path):
        return {"ok": False, "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {full_file_path}", "statistics": None}
    
    result = get_document_statistics(
        file_path=full_file_path,
        task_id=req.task_id,
        agent_user_id=req.agentUserId,
        output_file_path=req.output_file_path
    )
    
    return result

if __name__ == "__main__":
    if RUN_HTTP_SERVER:
        import uvicorn
        module_name = os.path.splitext(os.path.basename(__file__))[0]
        print(f"å¯åŠ¨æœåŠ¡: {SERVER_HOST}:{SERVER_PORT}")
        uvicorn.run(f"{module_name}:app", host=SERVER_HOST, port=SERVER_PORT, workers=SERVER_WORKERS)
