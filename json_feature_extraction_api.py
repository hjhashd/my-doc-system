# -*- coding: utf-8 -*-
import json
import re
import shutil
from pathlib import Path
import pandas as pd
from urllib.parse import quote

from docx import Document
from PIL import Image
import os

from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.opc.constants import RELATIONSHIP_TYPE as RT
from docx.shared import Pt, RGBColor

# ====== å¯è°ƒé˜ˆå€¼ ======
LARGE_TEXT_MIN_W = 500   # æŠŠ text çº æ­£ä¸º image çš„æœ€å°å®½
LARGE_TEXT_MIN_H = 400   # æŠŠ text çº æ­£ä¸º image çš„æœ€å°é«˜
CONTAIN_PAD = 2          # image åŒ…å«åˆ¤æ–­çš„åƒç´ å®¹å·®
IOU_DROP = 0.9           # image IoU è¶…è¿‡è¯¥é˜ˆå€¼åˆ™ä¿ç•™æ›´å¤§çš„ä¸€å¼ 


# ====== å­—ä½“ï¼šå®‹ä½“è®¾ç½® ======
def set_doc_font_simsun(doc: Document, font_name: str = "SimSun"):
    """
    å°†æ–‡æ¡£é»˜è®¤æ ·å¼ï¼ˆNormalï¼‰ä¸è¶…é“¾æ¥æ ·å¼ï¼ˆHyperlinkï¼‰è®¾ç½®ä¸ºå®‹ä½“ã€‚
    åŒæ—¶è®¾ç½® eastAsia æ—ï¼Œç¡®ä¿ä¸­æ–‡æ˜¾ç¤ºä¸ºå®‹ä½“ã€‚
    """
    # Normal
    try:
        normal = doc.styles["Normal"]
        normal.font.name = font_name
        # eastAsia
        normal._element.rPr.rFonts.set(qn("w:eastAsia"), font_name)
        # è‹±æ–‡/è¥¿æ–‡
        normal.font._element.rPr.rFonts.set(qn("w:ascii"), font_name)
        normal.font._element.rPr.rFonts.set(qn("w:hAnsi"), font_name)
    except Exception:
        pass

    # Hyperlinkï¼ˆæœ‰äº›æ¨¡æ¿å¯èƒ½æ²¡æœ‰è¯¥æ ·å¼ï¼Œåš try ä¿æŠ¤ï¼‰
    try:
        hyperlink = doc.styles["Hyperlink"]
        hyperlink.font.name = font_name
        hyperlink._element.rPr.rFonts.set(qn("w:eastAsia"), font_name)
        hyperlink._element.rPr.rFonts.set(qn("w:ascii"), font_name)
        hyperlink._element.rPr.rFonts.set(qn("w:hAnsi"), font_name)
    except Exception:
        pass


# ====== è¶…é“¾æ¥ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰ ======
def add_hyperlink(paragraph, text: str, target_ref: str, font_name: str = "SimSun"):
    """
    å‘æ®µè½ paragraph æ’å…¥ä¸€ä¸ªè¶…é“¾æ¥ï¼Œæ–‡æœ¬æ˜¾ç¤ºä¸º textï¼Œé“¾æ¥ç›®æ ‡ä½¿ç”¨ target_refï¼ˆç›¸å¯¹è·¯å¾„å­—ç¬¦ä¸²ï¼‰ã€‚
    æ³¨æ„ï¼šè¿™é‡Œä¸å†ä½¿ç”¨ file:/// ç»å¯¹ URIï¼Œè€Œæ˜¯ç›´æ¥æŠŠç›¸å¯¹è·¯å¾„å†™è¿›å¤–éƒ¨å…³ç³»ä¸­ï¼Œ
    Word ä¼šä»¥ .docx æ‰€åœ¨ç›®å½•ä¸ºåŸºå‡†è§£æå®ƒã€‚
    """
    part = paragraph.part
    # target_ref ä¾‹å¦‚ "img/XA_certificate_0_layout_det_res_1.png"
    r_id = part.relate_to(target_ref, RT.HYPERLINK, is_external=True)

    # <w:hyperlink r:id="...">
    hyperlink = OxmlElement('w:hyperlink')
    hyperlink.set(qn('r:id'), r_id)

    # <w:r><w:rPr>...</w:rPr><w:t>text</w:t></w:r>
    new_run = OxmlElement('w:r')
    rPr = OxmlElement('w:rPr')

    # ä½¿ç”¨ Hyperlink æ ·å¼ï¼ˆè“è‰²+ä¸‹åˆ’çº¿ï¼‰
    rStyle = OxmlElement('w:rStyle')
    rStyle.set(qn('w:val'), 'Hyperlink')
    rPr.append(rStyle)

    # æ˜¾å¼è®¾ç½®å­—ä½“æ—ä¸ºå®‹ä½“
    rFonts = OxmlElement('w:rFonts')
    rFonts.set(qn('w:eastAsia'), font_name)
    rFonts.set(qn('w:ascii'), font_name)
    rFonts.set(qn('w:hAnsi'), font_name)
    rPr.append(rFonts)

    new_run.append(rPr)

    t = OxmlElement('w:t')
    t.text = text
    new_run.append(t)

    hyperlink.append(new_run)
    paragraph._p.append(hyperlink)
    return paragraph


# ====== åŸºç¡€å·¥å…· ======
def load_json(json_path: Path):
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)


def get_page_tag(data: dict):
    input_path = data["input_path"]
    page_index = data["page_index"]
    pdf_name = Path(input_path).name
    base_name = pdf_name.rsplit(".", 1)[0]
    page_tag = f"{base_name}_{page_index}"
    return base_name, page_index, page_tag


def collect_header_bands(parsing_res_list):
    bands = []
    for blk in parsing_res_list:
        if blk.get("block_label") == "header":
            x1, y1, x2, y2 = blk["block_bbox"]
            bands.append((float(y1), float(y2)))
    return bands


def _y_overlap_ratio(a, b):
    a1, a2 = min(a), max(a)
    b1, b2 = min(b), max(b)
    inter = max(0.0, min(a2, b2) - max(a1, b1))
    denom = max(1e-6, min(a2 - a1, b2 - b1))
    return inter / denom


def is_in_header_band(block, header_bands, overlap_ratio_thresh=0.5):
    if not header_bands:
        return False
    _, y1, _, y2 = block["block_bbox"]
    for hy1, hy2 in header_bands:
        if _y_overlap_ratio((y1, y2), (hy1, hy2)) >= overlap_ratio_thresh:
            return True
    return False


def ensure_dir(path: Path):
    path.mkdir(parents=True, exist_ok=True)


def crop_image_region(layout_png_path: Path, bbox, out_path: Path):
    if not layout_png_path.exists():
        print(f"[WARN] layout image not found: {layout_png_path}")
        return False

    img = Image.open(layout_png_path)
    x1, y1, x2, y2 = bbox
    x1 = max(0, int(x1)); y1 = max(0, int(y1))
    x2 = min(img.width, int(x2)); y2 = min(img.height, int(y2))
    if x2 <= x1 or y2 <= y1:
        print(f"[WARN] invalid bbox, skip crop: {bbox}")
        return False

    cropped = img.crop((x1, y1, x2, y2))
    cropped.save(out_path)
    return True


# ====== æ–‡æ¡£ä¸è·¯å¾„å¤„ç† ======
def copy_table_file(asset_base_dir: Path, page_tag: str, table_idx: int, word_doc: Document, 
                    table_dir: Path, link_base_dir: Path, url_prefix: str = ""):
    """
    å°† Excel å¤åˆ¶åˆ°è¾“å‡ºç›®å½•ï¼Œç”Ÿæˆç¾åŒ–é“¾æ¥ + éšå½¢æ–‡ä»¶åé”šç‚¹
    """
    src_name = f"{page_tag}_table_{table_idx}.xlsx"
    src_path = asset_base_dir / src_name

    ensure_dir(table_dir)
    dst_path = table_dir / src_name

    # 1. ç‰©ç†å¤åˆ¶
    if src_path.exists():
        shutil.copy2(src_path, dst_path)
    else:
        print(f"[WARN] table xlsx not found: {src_path}")
        open(dst_path, "a").close()

    # 2. ç”Ÿæˆ Web è·³è½¬é“¾æ¥
    if url_prefix:
        parts = url_prefix.strip("/").split("/")
        agent_user_id = parts[1] if len(parts) >= 3 else ""
        task_id = parts[2] if len(parts) >= 3 else ""
        prefix = url_prefix.rstrip("/")
        file_web_path = f"{prefix}/table/{src_name}"
        encoded_url = quote(file_web_path)
        encoded_name = quote(src_name)
        rel_target = f"/excel-editor?docUrl={encoded_url}&docName={encoded_name}&agentUserId={agent_user_id}&taskId={task_id}"
    else:
        rel = os.path.relpath(dst_path, start=link_base_dir)
        rel_target = rel.replace("\\", "/")   

    # 3. å†™å…¥ Word (å…³é”®ä¿®æ”¹)
    p = word_doc.add_paragraph()
    
    # A. ã€ç»™äººçœ‹ã€‘ç¾åŒ–åçš„æ–‡å­—
    link_text = f"ï¿½ ç‚¹å‡»ç¼–è¾‘è¡¨æ ¼ ({src_name})"
    add_hyperlink(p, link_text, rel_target)

    # B. ã€ç»™ç¨‹åºçœ‹ã€‘æ·»åŠ éšå½¢ç­¾å {{#T#:æ–‡ä»¶å}}
    hidden_signature = f"{{{{#T#:{src_name}}}}}"  
    run = p.add_run(hidden_signature)
    
    # ====== ä¿®æ”¹å¼€å§‹ï¼šçœŸæ­£çš„è§†è§‰éšè— (ç™½è‰² + æå°å­—å·) ====== 
    run.font.size = Pt(0.5)                       # å­—å·è®¾ä¸º 0.5 ç£… (è‚‰çœ¼å‡ ä¹ä¸å¯è§) 
    run.font.color.rgb = RGBColor(255, 255, 255)  # é¢œè‰²è®¾ä¸ºç™½è‰² (èƒŒæ™¯ä¹Ÿæ˜¯ç™½çš„è¯å°±çœ‹ä¸è§äº†) 
    # ====== ä¿®æ”¹ç»“æŸ ====== 
    
    # è®¾ç½®éšè—å±æ€§ (ä¿ç•™è¿™ä¸ªå±æ€§ï¼ŒåŒé‡ä¿é™©)
    rPr = run._element.get_or_add_rPr()
    vanish = OxmlElement('w:vanish')
    rPr.append(vanish)


def sort_blocks_reading_order(parsing_res_list, y_tol=10):
    """
    æŒ‰é˜…è¯»é¡ºåºæ’åºï¼šå…ˆ yï¼ˆåˆ†æ¡¶ï¼‰ï¼Œå† x å‡åº
    """
    def key_fn(blk):
        x1, y1, x2, y2 = blk.get("block_bbox", [0, 0, 0, 0])
        row = round(float(y1) / float(y_tol))
        return (row, float(x1))
    return sorted(parsing_res_list, key=key_fn)


def is_other_text_attached(title_blk, cand_blk, y_tol=10):
    """
    åˆ¤æ–­ other_text æ˜¯å¦åº”é™„åŠ åˆ° paragraph_titleï¼š
      - y é«˜åº¦é‡å  > 50%
      - æˆ–è€… ä¸­çº¿è·ç¦» <= 2 * y_tol
    """
    _, ty1, _, ty2 = title_blk["block_bbox"]
    _, cy1, _, cy2 = cand_blk["block_bbox"]
    if _y_overlap_ratio((ty1, ty2), (cy1, cy2)) >= 0.5:
        return True
    if abs((ty1 + ty2) / 2.0 - (cy1 + cy2) / 2.0) <= 2 * y_tol:
        return True
    return False


# ====== bbox å·¥å…·å‡½æ•° ======
def _bbox_area(b):
    x1, y1, x2, y2 = map(float, b)
    return max(0.0, x2 - x1) * max(0.0, y2 - y1)


def _bbox_contains(outer, inner, pad=0.0):
    ox1, oy1, ox2, oy2 = map(float, outer)
    ix1, iy1, ix2, iy2 = map(float, inner)
    return (ix1 >= ox1 - pad and iy1 >= oy1 - pad and
            ix2 <= ox2 + pad and iy2 <= oy2 + pad)


def _bbox_iou(a, b):
    ax1, ay1, ax2, ay2 = map(float, a)
    bx1, by1, bx2, by2 = map(float, b)
    ix1, iy1 = max(ax1, bx1), max(ay1, by1)
    ix2, iy2 = min(ax2, bx2), min(ay2, by2)
    iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)
    inter = iw * ih
    aa = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)
    ba = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)
    union = aa + ba - inter + 1e-6
    return inter / union


# ====== è§„åˆ™ä¿®æ­£ï¼štext -> imageï¼ˆå¤§å—ï¼‰ ======
def coerce_large_text_to_image(parsing_res_list, min_w=LARGE_TEXT_MIN_W, min_h=LARGE_TEXT_MIN_H):
    new_list = []
    for blk in parsing_res_list:
        label = blk.get("block_label")
        if label == "text":
            x1, y1, x2, y2 = blk.get("block_bbox", [0, 0, 0, 0])
            w, h = float(x2 - x1), float(y2 - y1)
            if w >= min_w and h >= min_h:
                blk = dict(blk)
                blk["block_label"] = "image"
        new_list.append(blk)
    return new_list


# ====== ä»…ä¿ç•™æ›´å¤§ imageï¼šå»åŒ…å«/é«˜ IoU ======
def drop_nested_images(parsing_res_list, contain_pad=CONTAIN_PAD, iou_drop=IOU_DROP):
    images, others = [], []
    for blk in parsing_res_list:
        (images if blk.get("block_label") == "image" else others).append(blk)

    images_sorted = sorted(images, key=lambda b: _bbox_area(b["block_bbox"]), reverse=True)
    kept = []
    for b in images_sorted:
        bb = b["block_bbox"]
        drop = False
        for k in kept:
            kb = k["block_bbox"]
            if _bbox_contains(kb, bb, pad=contain_pad):
                drop = True
                break
            if _bbox_iou(kb, bb) >= iou_drop:
                drop = True
                break
        if not drop:
            kept.append(b)

    kept_set = set(id(x) for x in kept)
    images_kept_in_original_order = [blk for blk in images if id(blk) in kept_set]
    return images_kept_in_original_order + others


# ====== å¤„ç†å•ä¸ª JSON å¹¶è¿½åŠ åˆ° Word ======
def process_one_json_and_append(doc: Document, json_path: Path, img_dir: Path, table_dir: Path, link_base_dir: Path, url_prefix: str = ""):
    data = load_json(json_path)
    base_name, page_index, page_tag = get_page_tag(data)

    print(f"processing JSON: {json_path}")
    print(f"base_name = {base_name}, page_index = {page_index}, page_tag = {page_tag}")

    parsing_res_list = data.get("parsing_res_list", [])

    # è§„åˆ™ä¿®æ­£ + æ’åº
    parsing_res_list = coerce_large_text_to_image(parsing_res_list)
    parsing_res_list = drop_nested_images(parsing_res_list)
    parsing_res_list = sort_blocks_reading_order(parsing_res_list, y_tol=10)

    header_bands = collect_header_bands(parsing_res_list)

    # ä»¥ JSON æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸ºåŸºå‡†å¯»æ‰¾èµ„æº
    asset_base_dir = json_path.parent
    layout_png_path = asset_base_dir / f"{page_tag}_layout_det_res.png"

    image_idx = 1
    table_idx = 1
    i = 0
    n = len(parsing_res_list)

    ensure_dir(img_dir)
    ensure_dir(table_dir)

    while i < n:
        blk = parsing_res_list[i]
        label = blk.get("block_label")
        content = blk.get("block_content", "") or ""

        if label == "header" or is_in_header_band(blk, header_bands, overlap_ratio_thresh=0.5) or label == "seal":
            i += 1
            continue

        if label == "paragraph_title":
            merged_text = re.sub(r"\s+", "", content or "")
            j = i + 1
            while j < n:
                next_blk = parsing_res_list[j]
                if (next_blk.get("block_label") == "other_text"
                    and not is_in_header_band(next_blk, header_bands, overlap_ratio_thresh=0.5)
                    and is_other_text_attached(blk, next_blk, y_tol=10)):
                    merged_text += re.sub(r"\s+", "", next_blk.get("block_content", "") or "")
                    j += 1
                else:
                    break
            doc.add_paragraph(merged_text)
            i = j
            continue

        if label == "text":
            merged_text = content
            j = i + 1
            while j < n:
                next_blk = parsing_res_list[j]
                if (next_blk.get("block_label") == "other_text"
                    and not is_in_header_band(next_blk, header_bands, overlap_ratio_thresh=0.5)
                    and is_other_text_attached(blk, next_blk, y_tol=10)):
                    merged_text += next_blk.get("block_content", "") or ""
                    j += 1
                else:
                    break
            doc.add_paragraph(merged_text)
            i = j
            continue

        if label == "image":
            out_name = f"{page_tag}_layout_det_res_{image_idx}.png"
            out_path = img_dir / out_name
            if crop_image_region(layout_png_path, blk["block_bbox"], out_path):
                # 1. è®¡ç®—é“¾æ¥ç›®æ ‡
                if url_prefix:
                    prefix = url_prefix.rstrip("/")
                    rel_target = f"{prefix}/img/{out_name}"
                else:
                    rel = os.path.relpath(out_path, start=link_base_dir)
                    rel_target = rel.replace("\\", "/")
                
                p = doc.add_paragraph()
                
                # A. ã€ç»™äººçœ‹ã€‘ç¾åŒ–åçš„æ–‡å­—
                visible_text = f"ğŸ–¼ï¸ ç‚¹å‡»æŸ¥çœ‹å›¾ç‰‡ ({out_name})"
                add_hyperlink(p, visible_text, rel_target)
                
                # B. ã€ç»™ç¨‹åºçœ‹ã€‘æ·»åŠ éšå½¢ç­¾å {{#I#:æ–‡ä»¶å}}
                hidden_signature = f"{{{{#I#:{out_name}}}}}"
                run = p.add_run(hidden_signature)
                
                # ====== ä¿®æ”¹å¼€å§‹ï¼šçœŸæ­£çš„è§†è§‰éšè— (ç™½è‰² + æå°å­—å·) ====== 
                run.font.size = Pt(0.5)                       # å­—å·è®¾ä¸º 0.5 ç£… 
                run.font.color.rgb = RGBColor(255, 255, 255)  # é¢œè‰²è®¾ä¸ºç™½è‰² 
                # ====== ä¿®æ”¹ç»“æŸ ====== 
                
                # è®¾ç½®éšè—å±æ€§
                rPr = run._element.get_or_add_rPr()
                vanish = OxmlElement('w:vanish')
                rPr.append(vanish)

                image_idx += 1
            else:
                if content.strip():
                    doc.add_paragraph(content.strip())
            i += 1
            continue

        if label == "table":
            # ====== [ä¿®æ”¹ç‚¹] ä¼ é€’ url_prefix ç»™è¡¨æ ¼å‡½æ•° ====== 
            copy_table_file(asset_base_dir, page_tag, table_idx, doc, table_dir, 
                            link_base_dir=link_base_dir, url_prefix=url_prefix)
            # ================================================ 
            table_idx += 1
            i += 1
            continue

        if content.strip():
            doc.add_paragraph(content.strip())

        i += 1

    return base_name


def parse_file_sort_key(p: Path):
    """
    è§£æ *_res.json çš„æ’åºé”®ï¼š(base_name, index)
    è‹¥æ–‡ä»¶ååŒ¹é…å¤±è´¥ï¼Œå›é€€è¯»å– JSON ä¸­çš„ page_index
    """
    m = re.match(r"^(?P<base>.+)_(?P<idx>\d+)_res\.json$", p.name)
    if m:
        base = m.group("base")
        idx = int(m.group("idx"))
        return (base, idx)
    try:
        data = load_json(p)
        base_name, page_index, _ = get_page_tag(data)
        return (base_name, int(page_index))
    except Exception:
        return (p.stem, 0)


# ====== å¯å¤ç”¨ç”Ÿæˆå‡½æ•° ======
def generate_word_from_jsons(json_dir: Path, img_dir: Path, table_dir: Path, out_docx_dir: Path, url_prefix: str = ""):
    """
    æ‰«æ json_dir ä¸‹çš„ *_res.jsonï¼ŒæŒ‰é¡µåºåˆå¹¶å†™å…¥ä¸€ä¸ª Wordã€‚
    å›¾ç‰‡è£å‰ªä¿å­˜åˆ° img_dirï¼Œè¡¨æ ¼å¤åˆ¶åˆ° table_dirï¼Œæœ€ç»ˆ Word ä¿å­˜åœ¨ out_docx_dirã€‚
    å¦‚æœæä¾›äº† url_prefixï¼Œè¶…é“¾æ¥ç›®æ ‡ä½¿ç”¨è¯¥å‰ç¼€å¼€å¤´çš„ç»å¯¹è·¯å¾„ï¼›å¦åˆ™ä½¿ç”¨ç›¸å¯¹è·¯å¾„ã€‚
    è¿”å› (out_docx_path, True)ã€‚
    """
    if not json_dir.exists() or not json_dir.is_dir():
        raise FileNotFoundError(f"not a directory: {json_dir}")

    json_files = sorted(json_dir.glob("*_res.json"), key=parse_file_sort_key)
    if not json_files:
        raise FileNotFoundError(f"no *_res.json under {json_dir}")

    ensure_dir(img_dir)
    ensure_dir(table_dir)
    ensure_dir(out_docx_dir)

    doc = Document()
    # ç»Ÿä¸€è®¾ç½®ä¸ºå®‹ä½“
    set_doc_font_simsun(doc, "SimSun")

    base_names_seen = []
    for jp in json_files:
        base_name = process_one_json_and_append(doc, jp, img_dir=img_dir, table_dir=table_dir,
                                                link_base_dir=out_docx_dir, url_prefix=url_prefix)
        base_names_seen.append(base_name)

    uniq_bases = list(dict.fromkeys(base_names_seen))
    out_name = f"{uniq_bases[0]}_res.docx" if len(uniq_bases) == 1 else "merged_res.docx"
    out_docx = out_docx_dir / out_name

    doc.save(out_docx)
    print(f"saved Word: {out_docx.resolve()}")
    return out_docx, True


# output_base_dir = "./pdf_output/"
# output_pdf_id_path = os.path.join(output_base_dir, "2")

# pdf_out_path = output_pdf_id_path
# file_name = "XA_certificate"
# json_dir = Path(pdf_out_path) / file_name
# img_dir =  Path(pdf_out_path+"/img")
# table_dir = Path(pdf_out_path+"/table")
# out_docx_dir = Path(pdf_out_path+"/")

# print(json_dir)
# print(img_dir)
# print(table_dir)
# print(out_docx_dir)

# generate_word_from_jsons(json_dir, img_dir, table_dir, out_docx_dir)