import os, json, traceback
from typing import Optional, Dict, Any, List, Callable
from fastapi import FastAPI, BackgroundTasks, Query
from pydantic import BaseModel, Field

# ===== æ˜¾å¼å¸¸é‡å®šä¹‰ =====
DEFAULT_OUTPUT_FILE_PATH: str = "/data/cwd_cq/out"
DEFAULT_INPUT_FILE_PATH: str = "/home/xzh/ocr_flie/pdf_output"
DEFAULT_RUN_PICTURE: bool = True
DEFAULT_USE_MERGED_FOR_MERGE: bool = True

# ===== å¯¼å…¥ä½ çš„æºæ–‡ä»¶ (ä¿æŒä¸å˜) =====
import title_recognition as mod_title_rec
import title_position_calculator as mod_pos
import clear_empty_blocks_manager as mod_clear
import table_recognition as mod_table
import table_title_completely_merge_with_content as mod_merge
from picture_recognition import EnhancedImageProcessor

# ===== æœåŠ¡å¸¸é‡ =====
RUN_HTTP_SERVER: bool = True
SERVER_HOST: str = "0.0.0.0"
SERVER_PORT: int = 8005
SERVER_WORKERS: int = 1

# ===== å…¨å±€ä»»åŠ¡çŠ¶æ€å­˜å‚¨ =====
# Key: "{agentUserId}_{taskId}", Value: Dict
GLOBAL_TASK_STORE: Dict[str, Dict[str, Any]] = {}


# ============== å·¥å…·å‡½æ•° (ä¿æŒä¸å˜) ==============
def _abspath(p: Optional[str]) -> Optional[str]:
    return os.path.abspath(p) if p else None


def _assert_file_exists(p: str, name: str):
    if not os.path.isfile(p):
        raise FileNotFoundError(f"{name} ä¸å­˜åœ¨: {p}")


def _build_output_dir(output_file_path: str, agent_user_id: int, task_id: str) -> str:
    output_dir = os.path.join(output_file_path, str(agent_user_id), str(task_id))
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


def _wrap_blocks_if_needed(merged_path: str, json_file_name: str, output_dir: str) -> str:
    with open(merged_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if isinstance(data, dict) and "blocks" in data:
        return merged_path
    if isinstance(data, list):
        wrapped = {"blocks": data}
        os.makedirs(output_dir, exist_ok=True)
        wrapped_path = os.path.join(output_dir, f"{json_file_name}_blocks_merge_wrapped.json")
        with open(wrapped_path, "w", encoding="utf-8") as wf:
            json.dump(wrapped, wf, ensure_ascii=False, indent=2)
        return _abspath(wrapped_path)
    return merged_path


# ============== æ ¸å¿ƒæµç¨‹ (ä¿æŒä¸å˜ï¼Œæ¥æ”¶å®Œæ•´è·¯å¾„) ==============
def run_pipeline_sync(
        file_path: str,  # è¿™é‡Œçš„ file_path æ˜¯æ‹¼æ¥å¥½åçš„å®Œæ•´è·¯å¾„
        task_id: str,
        agent_user_id: int,
        output_file_path: str,
        input_file_path: str = DEFAULT_INPUT_FILE_PATH,
        run_picture: bool = DEFAULT_RUN_PICTURE,
        use_merged_blocks_for_merge: bool = DEFAULT_USE_MERGED_FOR_MERGE,
        progress_callback: Callable[[str, int, str], None] = None
) -> Dict[str, Any]:
    def report_progress(step_key: str, percent: int, desc: str):
        if progress_callback:
            progress_callback(step_key, percent, desc)

    output_dir = _build_output_dir(output_file_path, agent_user_id, task_id)

    # åˆå§‹åŒ–è¿”å›ç»“æ„
    file_outputs = {
        "step1_title_recognition": {"step_name": "æ ‡é¢˜è¯†åˆ«", "file_path": None, "file_name": None,
                                    "description": "è¯†åˆ«æ–‡æ¡£ä¸­çš„æ ‡é¢˜åŠå…¶å±‚çº§ç»“æ„"},
        "step2_title_position_blocks": {"step_name": "æ ‡é¢˜å®šä½ä¸åˆ‡å—", "file_path": None, "file_name": None,
                                        "description": "æ ¹æ®æ ‡é¢˜ä½ç½®å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªå—"},
        "step3_blocks_merge": {"step_name": "ç©ºå—åˆå¹¶", "file_path": None, "file_name": None,
                               "description": "åˆå¹¶æ–‡æ¡£ä¸­çš„ç©ºç™½å—"},
        "step3_blocks_merge_wrapped": {"step_name": "ç©ºå—åˆå¹¶ï¼ˆåŒ…è£…ç‰ˆï¼‰", "file_path": None, "file_name": None,
                                       "description": "åŒ…è£…åçš„ç©ºå—åˆå¹¶ç»“æœ"},
        "step4_table_recognition": {"step_name": "è¡¨æ ¼è¯†åˆ«", "file_path": None, "file_name": None,
                                    "description": "è¯†åˆ«æ–‡æ¡£ä¸­çš„æ‰€æœ‰è¡¨æ ¼åŠå…¶å†…å®¹"},
        "step5_tables_with_heading": {"step_name": "è¡¨æ ¼æ ‡é¢˜åˆå¹¶", "file_path": None, "file_name": None,
                                      "description": "å°†æ ‡é¢˜å…ƒæ•°æ®åˆå¹¶åˆ°è¡¨æ ¼ä¿¡æ¯ä¸­"},
        "step6_picture_recognition": {"step_name": "å›¾ç‰‡è¯†åˆ«", "file_path": None, "file_name": None,
                                      "description": "è¯†åˆ«æ–‡æ¡£ä¸­çš„å›¾ç‰‡åŠå…³é”®ä¿¡æ¯"}
    }

    meta = {"document_name": None, "task_id": task_id, "agent_user_id": agent_user_id, "output_directory": output_dir,
            "input_file_path": input_file_path}
    log = []
    print('uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu')
    try:
        report_progress("init", 5, "åˆå§‹åŒ–ç¯å¢ƒ...")

        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        _assert_file_exists(file_path, "è¾“å…¥DOCXæ–‡ä»¶")
        log.append(f"âœ… å¤„ç†æ–‡ä»¶: {file_path}")
        log.append(f"âœ… è¾“å‡ºç›®å½•: {output_dir}")

        # Step 1
        report_progress("step1", 15, "æ­£åœ¨æ‰§è¡Œ: æ ‡é¢˜è¯†åˆ«...")
        log.append("ğŸ”„ Step 1: æ ‡é¢˜è¯†åˆ«...")
        text, json_file_name, title_json_path = mod_title_rec.main(file_path, output_dir=output_dir)
        if text is None or not json_file_name or not title_json_path: raise RuntimeError("Step 1 å¤±è´¥")
        file_outputs["step1_title_recognition"]["file_path"] = _abspath(title_json_path)
        file_outputs["step1_title_recognition"]["file_name"] = os.path.basename(title_json_path)
        meta["document_name"] = json_file_name

        # Step 2
        report_progress("step2", 30, "æ­£åœ¨æ‰§è¡Œ: æ ‡é¢˜å®šä½ä¸åˆ‡å—...")
        log.append("ğŸ”„ Step 2: æ ‡é¢˜å®šä½ä¸åˆ‡å—...")
        blocks_json_path = mod_pos.main(text, title_json_path, output_dir=output_dir)
        if not blocks_json_path: raise RuntimeError("Step 2 å¤±è´¥")
        file_outputs["step2_title_position_blocks"]["file_path"] = _abspath(blocks_json_path)
        file_outputs["step2_title_position_blocks"]["file_name"] = os.path.basename(blocks_json_path)

        # Step 3
        report_progress("step3", 45, "æ­£åœ¨æ‰§è¡Œ: ç©ºå—åˆå¹¶...")
        log.append("ğŸ”„ Step 3: ç©ºå—åˆå¹¶...")
        blocks_merge_json_path = mod_clear.main(json_file_name=json_file_name, input_blocks_path=blocks_json_path,
                                                output_dir=output_dir)
        if blocks_merge_json_path:
            file_outputs["step3_blocks_merge"]["file_path"] = _abspath(blocks_merge_json_path)
            file_outputs["step3_blocks_merge"]["file_name"] = os.path.basename(blocks_merge_json_path)

        blocks_for_merge = file_outputs["step2_title_position_blocks"]["file_path"]
        if use_merged_blocks_for_merge and blocks_merge_json_path:
            wrapped_path = _wrap_blocks_if_needed(blocks_merge_json_path, json_file_name, output_dir)
            file_outputs["step3_blocks_merge_wrapped"]["file_path"] = wrapped_path
            file_outputs["step3_blocks_merge_wrapped"]["file_name"] = os.path.basename(wrapped_path)
            blocks_for_merge = wrapped_path

        # Step 4
        report_progress("step4", 60, "æ­£åœ¨æ‰§è¡Œ: è¡¨æ ¼è¯†åˆ«...")
        log.append("ğŸ”„ Step 4: è¡¨æ ¼è¯†åˆ«...")
        tables_json_path = mod_table.main(text, json_file_name=json_file_name, output_dir=output_dir,
                                          input_file_path=input_file_path, agent_user_id=agent_user_id, task_id=task_id)
        if not tables_json_path: raise RuntimeError("Step 4 å¤±è´¥")
        file_outputs["step4_table_recognition"]["file_path"] = _abspath(tables_json_path)
        file_outputs["step4_table_recognition"]["file_name"] = os.path.basename(tables_json_path)

        # Step 5
        report_progress("step5", 75, "æ­£åœ¨æ‰§è¡Œ: è¡¨æ ¼æ ‡é¢˜åˆå¹¶...")
        log.append("ğŸ”„ Step 5: è¡¨æ ¼æ ‡é¢˜åˆå¹¶...")
        tables_with_heading_json_path = mod_merge.main(json_file_name=json_file_name, tables_json_path=tables_json_path,
                                                       blocks_json_path=blocks_for_merge, output_dir=output_dir)
        if not tables_with_heading_json_path: raise RuntimeError("Step 5 å¤±è´¥")
        file_outputs["step5_tables_with_heading"]["file_path"] = _abspath(tables_with_heading_json_path)
        file_outputs["step5_tables_with_heading"]["file_name"] = os.path.basename(tables_with_heading_json_path)

        # Step 6
        report_progress("step6", 90, "æ­£åœ¨æ‰§è¡Œ: å›¾ç‰‡è¯†åˆ«...")
        if run_picture:
            log.append("ğŸ”„ Step 6: å›¾ç‰‡è¯†åˆ«...")
            proc = EnhancedImageProcessor(input_file_path=input_file_path, agent_user_id=agent_user_id, task_id=task_id)
            results = proc.process_text_with_images(text)
            picture_json_path = proc.save_results(results=results, output_file=f"{json_file_name}_picture.json",
                                                  output_dir=output_dir)
            file_outputs["step6_picture_recognition"]["file_path"] = _abspath(picture_json_path)
            file_outputs["step6_picture_recognition"]["file_name"] = os.path.basename(picture_json_path)
        else:
            log.append("â­ï¸ Step 6: å›¾ç‰‡è¯†åˆ«å·²è·³è¿‡")

        report_progress("finished", 100, "å¤„ç†å®Œæˆ")
        log.append("ğŸ‰ æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆï¼")

        return {
            "ok": True, "status": 1, "message": "æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ",
            "file_outputs": file_outputs, "meta": meta, "log": log,
            "config": {"output_directory": output_dir}
        }

    except Exception as e:
        log.append(f"âŒ å¼‚å¸¸: {str(e)}")
        return {
            "ok": False, "status": 0, "message": f"{type(e).__name__}: {e}",
            "file_outputs": file_outputs, "meta": meta,
            "log": log + [traceback.format_exc()],
            "config": {"output_directory": output_dir}
        }


# ============== ğŸ”¥ FastAPI æ¥å£ä¸æ¨¡å‹ä¿®æ”¹ ğŸ”¥ ==============
app = FastAPI(title="Doc Pipeline API", version="1.7.0", docs_url="/docs")


class PipelineRequest(BaseModel):
    # ğŸ”¥ ä¿®æ”¹ç‚¹ 1: file_path ç§»é™¤äº†ï¼Œæ”¹ä¸º file_name
    file_name: str = Field(..., description="æ–‡ä»¶åç§° (ä¾‹å¦‚ test.docx)", example="è¯ä¹¦æ–‡æ¡£.docx")
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    agentUserId: int = Field(..., description="ä»£ç†ç”¨æˆ·ID")
    output_file_path: str = Field(default=DEFAULT_OUTPUT_FILE_PATH, description="ç»“æœè¾“å‡ºç›®å½•")
    input_file_path: str = Field(default=DEFAULT_INPUT_FILE_PATH, description="æ–‡ä»¶æ‰€åœ¨çš„æ ¹ç›®å½•")


class RunResponse(BaseModel):
    ok: bool
    message: str
    query_id: str
    status_url: str


class StatusResponse(BaseModel):
    ok: bool
    status: str
    percent: int
    message: str
    result: Optional[Dict[str, Any]] = None


# ============== åå°ä»»åŠ¡åŒ…è£…å™¨ (å¤„ç†è·¯å¾„æ‹¼æ¥) ==============
def background_process_wrapper(req_data: 'PipelineRequest', unique_key: str):
    """
    åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œè´Ÿè´£æ‹¼æ¥è·¯å¾„å¹¶è°ƒç”¨æ ¸å¿ƒé€»è¾‘
    """
    # 1. åˆå§‹åŒ–çŠ¶æ€
    GLOBAL_TASK_STORE[unique_key] = {
        "status": "running", "percent": 0, "current_step": "init",
        "message": "ä»»åŠ¡å·²å¯åŠ¨", "result": None
    }

    def update_store(step_key, percent, desc):
        if unique_key in GLOBAL_TASK_STORE:
            GLOBAL_TASK_STORE[unique_key]["percent"] = percent
            GLOBAL_TASK_STORE[unique_key]["current_step"] = step_key
            GLOBAL_TASK_STORE[unique_key]["message"] = desc

    try:

        full_file_path = os.path.join(req_data.input_file_path, str(req_data.task_id), req_data.file_name)

        # æ‰“å°ä¸€ä¸‹è·¯å¾„ç¡®è®¤ (å¯é€‰ï¼Œç”¨äºè°ƒè¯•)
        print(f"Processing file: {full_file_path}")

        final_result = run_pipeline_sync(
            file_path=full_file_path,  # ä¼ å…¥æ‹¼æ¥å¥½çš„å« task_id çš„ç»å¯¹è·¯å¾„
            task_id=req_data.task_id,
            agent_user_id=req_data.agentUserId,
            output_file_path=req_data.output_file_path,
            input_file_path=req_data.input_file_path, # ä¼ ç»™å†…éƒ¨æ¨¡å—çš„ä¾ç„¶æ˜¯æ ¹è·¯å¾„ï¼Œå†…éƒ¨æ¨¡å—é€šå¸¸ä¼šè‡ªå·±å†æ‹¼ä¸€æ¬¡ ID
            run_picture=DEFAULT_RUN_PICTURE,
            use_merged_blocks_for_merge=DEFAULT_USE_MERGED_FOR_MERGE,
            progress_callback=update_store
        )

        status_str = "success" if final_result["status"] == 1 else "failed"
        GLOBAL_TASK_STORE[unique_key].update({
            "status": status_str, "percent": 100, "current_step": "finished",
            "message": final_result["message"], "result": final_result
        })

    except Exception as e:
        # æ‰“å°å †æ ˆä»¥ä¾¿è°ƒè¯•
        traceback.print_exc()
        GLOBAL_TASK_STORE[unique_key].update({
            "status": "failed", "percent": 100, "message": f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}", "result": None
        })


# ğŸ”¥ æäº¤ä»»åŠ¡æ¥å£
@app.post("/pipeline/run", response_model=RunResponse)
async def run_pipeline(req: PipelineRequest, background_tasks: BackgroundTasks):
    unique_key = f"{req.agentUserId}_{req.task_id}"

    if unique_key in GLOBAL_TASK_STORE and GLOBAL_TASK_STORE[unique_key]["status"] == "running":
        return {
            "ok": True, "message": "ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­", "query_id": unique_key,
            "status_url": f"/pipeline/status?query_id={unique_key}"
        }

    background_tasks.add_task(background_process_wrapper, req, unique_key)

    return {
        "ok": True, "message": "ä»»åŠ¡å·²æ¥æ”¶ï¼Œæ­£åœ¨åå°å¤„ç†", "query_id": unique_key,
        "status_url": f"/pipeline/status?query_id={unique_key}"
    }


# ğŸ”¥ è½®è¯¢æ¥å£
@app.get("/pipeline/status", response_model=StatusResponse)
async def get_pipeline_status(query_id: str = Query(..., description="ä»»åŠ¡çš„å”¯ä¸€ID")):
    task_info = GLOBAL_TASK_STORE.get(query_id)
    if not task_info:
        return {"ok": False, "status": "not_found", "percent": 0, "message": "ä»»åŠ¡ä¸å­˜åœ¨", "result": None}
    return {
        "ok": True, "status": task_info["status"], "percent": task_info["percent"],
        "message": task_info["message"], "result": task_info["result"]
    }


if __name__ == "__main__":
    if RUN_HTTP_SERVER:
        import uvicorn
        module_name = os.path.splitext(os.path.basename(__file__))[0]
        print(f"å¯åŠ¨æœåŠ¡: {SERVER_HOST}:{SERVER_PORT}")
        uvicorn.run(f"{module_name}:app", host=SERVER_HOST, port=SERVER_PORT, workers=SERVER_WORKERS)

